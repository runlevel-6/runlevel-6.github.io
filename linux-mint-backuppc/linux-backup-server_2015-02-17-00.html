<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1 plus MathML 2.0//EN" "http://www.w3.org/Math/DTD/mathml2/xhtml-math11-f.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><!--This file was converted to xhtml by LibreOffice - see http://cgit.freedesktop.org/libreoffice/core/tree/filter/source/xslt for the code.--><head profile="http://dublincore.org/documents/dcmi-terms/"><meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8"/><title xml:lang="en-US">Linux Mint 17.1 setup as a BackupPC server</title><meta name="DCTERMS.title" content="Linux Mint 17.1 setup as a BackupPC server" xml:lang="en-US"/><meta name="DCTERMS.language" content="en-US" scheme="DCTERMS.RFC4646"/><meta name="DCTERMS.source" content="http://xml.openoffice.org/odf2xhtml"/><meta name="DCTERMS.contributor" content="kzahorec "/><meta name="DCTERMS.modified" content="2015-03-01T13:01:11.900141165" scheme="DCTERMS.W3CDTF"/><meta name="DCTERMS.provenance" content="" xml:lang="en-US"/><meta name="DCTERMS.subject" content="," xml:lang="en-US"/><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/" hreflang="en"/><link rel="schema.DCTERMS" href="http://purl.org/dc/terms/" hreflang="en"/><link rel="schema.DCTYPE" href="http://purl.org/dc/dcmitype/" hreflang="en"/><link rel="schema.DCAM" href="http://purl.org/dc/dcam/" hreflang="en"/><style type="text/css">
	@page {  }
	table { border-collapse:collapse; border-spacing:0; empty-cells:show }
	td, th { vertical-align:top; font-size:12pt;}
	h1, h2, h3, h4, h5, h6 { clear:both }
	ol, ul { margin:0; padding:0;}
	li { list-style: none; margin:0; padding:0;}
	<!-- "li span.odfLiEnd" - IE 7 issue-->
	li span. { clear: both; line-height:0; width:0; height:0; margin:0; padding:0; }
	span.footnodeNumber { padding-right:1em; }
	span.annotation_style_by_filter { font-size:95%; font-family:Arial; background-color:#fff000;  margin:0; border:0; padding:0;  }
	* { margin:0;}
	.P1 { font-size:10pt; margin-bottom:0in; margin-top:0in; font-family:Liberation Mono; writing-mode:page; }
	<!-- ODF styles with no properties representable as CSS -->
	 { }
	</style></head><body dir="ltr" style="max-width:8.5in;margin-top:0.7874in; margin-bottom:0.7874in; margin-left:0.7874in; margin-right:0.7874in; writing-mode:lr-tb; "><p class="P1">Basic instruction for Linux Mint 17.1 setup as a BackupPC server</p><p class="P1">for hypervisor client VMs.</p><p class="P1"> </p><p class="P1">Author: Ken Zahorec</p><p class="P1">date: 2015-02-17-00</p><p class="P1"> </p><p class="P1">=========================================================================</p><p class="P1"> </p><p class="P1">--- System Hardware planning.</p><p class="P1">I recommend a decent multi-core 64-bit Intel or AMD system. This could be</p><p class="P1">anything from an enterprise grade server to a moderate desktop system. A lower</p><p class="P1">end 32-bit system might work as well, but I have not set one up and do not plan</p><p class="P1">to do so.</p><p class="P1"> </p><p class="P1">Planning the hard drive and boot arrangement can simplify setup later in the</p><p class="P1">process. In this case we are using two 1TB hard drives to create a single large</p><p class="P1">data store volume, and then go on with instructions on how to use them.</p><p class="P1"> </p><p class="P1">Alternatively a single drive could be used. Use of LVM, during the Linux Mint</p><p class="P1">installation, is suggested either way. LVM provides the abilty to easily expand</p><p class="P1">storage across multiple hard drives such that it looks like a single</p><p class="P1">volume--when it becomes necessary to do so.</p><p class="P1"> </p><p class="P1">Be careful of boot ordering in the BIOS. Use the first enumerated drive as</p><p class="P1">first boot order hard drive device.  Optionally, for convenience and</p><p class="P1">simplicity, adjust drive connections to mainboard or drive slots.  Arrange</p><p class="P1">drives such that the desired host system drive will be first device /dev/sda</p><p class="P1"> </p><p class="P1">drive 1: smaller hard drive, for pc-backup host system drive, will become /dev/sda</p><p class="P1">drive 2: large hard drive, planned for backup storage, will become /dev/sdb</p><p class="P1">drive 3: large hard drive, planned for backup storage, will become /dev/sdc</p><p class="P1"> </p><p class="P1">Insert all drives into the system including the planned storage drives.</p><p class="P1"> </p><p class="P1">Storage requirements for backuppc server</p><p class="P1"> </p><p class="P1">Generally speaking, and with information from limited testing, we are finding</p><p class="P1">that we require about 1/5 to 1/10 unit amount of backup storage for each unit</p><p class="P1">of client storage. Needs may differ for others of course.  This means that in</p><p class="P1">order to backup 150GB of VMs will require about 15GB to 30GB of storage volume</p><p class="P1">at the backuppc storage host.  We could conceivably and safely backup 5 1TB</p><p class="P1">virtualization servers, with moderate VM loads, using a single 1TB storage</p><p class="P1">drive.  The solution can be used to directly backup server, desktop, and laptop</p><p class="P1">hosts as well. We are not sure about the storage needs in this regards, but</p><p class="P1">based on the de-dupe and compression it should be relatively minimal--more</p><p class="P1">efficient than backing up full VM images.</p><p class="P1"> </p><p class="P1">--- Uninterruptible Power Supply (UPS) is recommended</p><p class="P1"> </p><p class="P1">Use of a UPS is recommended to avoid backup system interruptions during power</p><p class="P1">failures. One of easiest UPS systems to configure with Linux is an APC UPS</p><p class="P1">model with a USB connection. There is a driver and daemon "apcupsd" for APC</p><p class="P1">UPS models readily available from the default repositories. Some other</p><p class="P1">manufactures use the same USB interface as the APC units.</p><p class="P1"> </p><p class="P1">========================================================================</p><p class="P1"> </p><p class="P1">--- Basic Installation of Linux Mint.</p><p class="P1"> </p><p class="P1">Install Mint 17.1 Cinnamon 64-bit edition</p><p class="P1"> </p><p class="P1">Run full desktop installation</p><p class="P1"> - Boot the installation media to a running trial desktop</p><p class="P1"> - Double click the installation object on the desktop</p><p class="P1"> </p><p class="P1">To start installation from behind a proxy server (if necessary):</p><p class="P1"> - Boot the installation media to a running trial desktop</p><p class="P1"> - Set Proxy in Menu - Preferences - Network</p><p class="P1"> - On desktop, get command line properties of installer, sh -c 'ubiquity gtk_ui'</p><p class="P1"> - Open a terminal and use sudo -E with the command from above</p><p class="P1"><span> </span><span> Use sudo -E sh -c 'ubiquity gtk_ui'</span></p><p class="P1"><span> </span></p><p class="P1">During the install, Make sure to use the required system destination drive, not</p><p class="P1">a planned storage drive.  Use defaults, but additionally also include LVM for</p><p class="P1">main system.  We want LVM packages added to the system.</p><p class="P1"> </p><p class="P1">----------------------------------------------</p><p class="P1"> </p><p class="P1">--- Linux Mint setup items</p><p class="P1"> </p><p class="P1">System Settings - Effects</p><p class="P1"> </p><p class="P1"> - Disable all desktop effects (uncheck), we don't really need special effects</p><p class="P1">   display and processor loading on a backup server.</p><p class="P1"> </p><p class="P1">System Settings - Network</p><p class="P1"> </p><p class="P1"> - Set manual fixed network address</p><p class="P1"> - Set DNS server(s) (optional)</p><p class="P1"> </p><p class="P1"> - If business proxy is required, set it as needed.</p><p class="P1"><span> Menu - Preferences - Network - Proxy</span></p><p class="P1"> </p><p class="P1">If proxy is used, then all sudo commands using apt-get from this point</p><p class="P1">on will require the "-E" option to get the users environment passed on.  With</p><p class="P1">this approach, you will only get the proxy settings in your environment if you</p><p class="P1">run gnome-terminal.  This means that if you remotely access the server using</p><p class="P1">ssh -X, then you will need to additionally start gnome-terminal to use apt-get.</p><p class="P1">This also means that you will need a local X server to display the graphical</p><p class="P1">gnome-terminal program UI.</p><p class="P1"> </p><p class="P1"><span> # gnome-terminal &amp;</span></p><p class="P1"> </p><p class="P1">Update system after initial installation is complete and you have rebooted the</p><p class="P1">host.</p><p class="P1"> </p><p class="P1"><span> sudo apt-get update</span></p><p class="P1"><span> sudo apt-get upgrade</span></p><p class="P1"> </p><p class="P1">Install graphical LVM management tool</p><p class="P1"> </p><p class="P1"><span> sudo apt-get install system-config-lvm</span></p><p class="P1"> </p><p class="P1">Install openssh server to provide remote access to host from ssh clients</p><p class="P1"> </p><p class="P1"><span> sudo apt-get install openssh-server</span></p><p class="P1"> </p><p class="P1">Optionally, after this step we can continue all of the remaining steps from</p><p class="P1">remote client login through ssh -X.  If you are using a proxy to get to</p><p class="P1">Internet, then this can be accomplished by first logging in to standard</p><p class="P1">terminal, THEN opening up gnome-terminal, a secondary terminal that contains</p><p class="P1">the proxy settings in the environment.</p><p class="P1"> </p><p class="P1"><span> ssh -X user@pc-backup_ip_address</span></p><p class="P1"><span> gnome-terminal &amp;</span></p><p class="P1"> </p><p class="P1">Create a temporary backup-storage mount directory in the system.  This will be</p><p class="P1">used as a target to copy the initial backuppc data before the mount point is</p><p class="P1">moved.</p><p class="P1"> </p><p class="P1"><span> sudo mkdir /mnt/backup-storage</span></p><p class="P1"> </p><p class="P1">--------------------------------------------</p><p class="P1"> </p><p class="P1">---Creating a very large capacity storage volume with a temporary mountpoint.</p><p class="P1"> </p><p class="P1">Use system-config-lvm to create the large data store for the backup server</p><p class="P1">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Storage_Administration_Guide/s1-system-config-lvm.html</p><p class="P1"> </p><p class="P1">Make sure any existing file systems from sdb and sdc are unmounted.  Unmount</p><p class="P1">them if you have to. Assuming we are not using these volumes for the system. If</p><p class="P1">you have them as part of the system, then go back and install again without</p><p class="P1">using them.</p><p class="P1"><span> </span></p><p class="P1">Menu - Administration - Logical Volume Management</p><p class="P1"> - Init /dev/sdb</p><p class="P1"> - Init /dev/sdc</p><p class="P1"> - Create a new backup storage logical volume group called "storage-vg"</p><p class="P1"> - Put both sdb and sdc into storage-vg</p><p class="P1"> - Create a new backup storage logical volume "storage-lv"</p><p class="P1"><span> Format storage-lv using XFS</span></p><p class="P1"><span> - Use all space</span></p><p class="P1"><span> - Apply checkbox to mount on next boot</span></p><p class="P1"><span> - Specify the mountpoint /mnt/backup-storage</span><span> </span></p><p class="P1"> </p><p class="P1">--------------------------------------------</p><p class="P1"> </p><p class="P1">---Install and configure backuppc solution.</p><p class="P1"> </p><p class="P1">https://www.digitalocean.com/community/tutorials/how-to-use-backuppc-to-create-a-backup-server-on-an-ubuntu-12-04-vps</p><p class="P1"> </p><p class="P1">Install backuppc enterprise grade backup solution</p><p class="P1"> </p><p class="P1">(Added this apache2-utils later on in the first attempt, might instead be</p><p class="P1">better here. Try it to confirm.)</p><p class="P1"> </p><p class="P1"><span> sudo apt-get install apache2-utils</span></p><p class="P1"><span> sudo apt-get install backuppc</span></p><p class="P1"> </p><p class="P1"><span> Postfix configuration popup during install...</span></p><p class="P1"><span> </span><span> use "Local only" for mail server type</span></p><p class="P1"> </p><p class="P1"><span> ...creates random password for apache e.g. "eWnYdU3a"</span></p><p class="P1"> </p><p class="P1">To later change password for apache2 access to something more friendly.  This</p><p class="P1">was required on the first try because apache2-utils appears to be a missing</p><p class="P1">dependency for backuppc install.</p><p class="P1"> </p><p class="P1"><span> sudo apt-get install apache2-utils (if not done previously above)</span></p><p class="P1"><span> sudo htpasswd /etc/backuppc/htpasswd backuppc</span></p><p class="P1"> </p><p class="P1">For running backuppc pre-dump command via rsync we will also need perl module</p><p class="P1">rsyncp.  Without this you will get errors in the log when the backup pre-dump</p><p class="P1">rsync command is attempted at the client.</p><p class="P1"> </p><p class="P1"><span> sudo apt-get install libfile-rsyncp-perl</span></p><p class="P1"> </p><p class="P1">--------------------------------------------</p><p class="P1"> </p><p class="P1">--- Adjusting the backuppc server's large storage volume mount point</p><p class="P1"><span> performing the "switcheroo"</span></p><p class="P1"> </p><p class="P1">The general idea here is that the filesystem at /var/lib/backuppc needs to be</p><p class="P1">mounted to the large storage volume.  We have to deal with the file system</p><p class="P1">objects that are currently at this directory before we make the switch.  We</p><p class="P1">could try to just config-change the backuppc storage location over to</p><p class="P1">/mnt/backup-storage, but this approach is not recommended.  We will instead</p><p class="P1">build the default backuppc storage location into our filesystem--relatively</p><p class="P1">easy to do.</p><p class="P1"> </p><p class="P1">" 1. mount - as already suggested - your backup file-system directly at the</p><p class="P1">mount point /var/lib/backuppc"</p><p class="P1"> </p><p class="P1">reference: http://sourceforge.net/p/backuppc/mailman/message/21267701/</p><p class="P1"> </p><p class="P1">The backup target directory /var/lib/backuppc is currently rooted in the root</p><p class="P1">"/" filesystem--not at the large LVM data store.  We need it mounted at the</p><p class="P1">large LVM data store created earlier to take advantage of our high capacity</p><p class="P1">hard drives.  The large data store volume is currently mounted at</p><p class="P1">/mnt/backup-storage. We need it mounted at /var/lib/backuppc.  We will stop the</p><p class="P1">backuppc daemon, copy /var/lib/backuppc files over to the current LVM data</p><p class="P1">store at /mnt/backup-storage.  We will then shift the mount point to</p><p class="P1">/var/lib/backuppc. This will change the backup target over to the large</p><p class="P1">LVM data store located at the new mount position /var/lib/backuppc with all of</p><p class="P1">the original file system data objects intact.  We are, in essence, pulling a</p><p class="P1">"switcheroo" on the backuppc storage area.</p><p class="P1"> </p><p class="P1">First stop the backuppc daemon...</p><p class="P1"><span> </span></p><p class="P1"><span> sudo /etc/init.d/backuppc stop</span></p><p class="P1"> </p><p class="P1">Copy the backuppc storage area over to the large volume...</p><p class="P1">The p preserves all file system attributes, v is verbose, and r is recursive.</p><p class="P1"> </p><p class="P1"><span> sudo cp -pvr /var/lib/backuppc/* /mnt/backup-storage/.</span></p><p class="P1"> </p><p class="P1">Now we can unmount the backup-storage by issuing the umount command as follows.</p><p class="P1"> </p><p class="P1"><span> sudo umount /mnt/backup-storage</span></p><p class="P1"> </p><p class="P1">Edit the mount point in /etc/fstab to use the new filesystem mount point at</p><p class="P1">/var/lib/backuppc (replace "/mnt/backup-storage" with "/var/lib/backuppc")</p><p class="P1"> </p><p class="P1"><span> sudo gedit /etc/fstab</span></p><p class="P1"> </p><p class="P1">The significant line in fstab should look like the following</p><p class="P1"> </p><p class="P1">  /dev/storage-vg/storage-lv   /var/lib/backuppc  xfs defaults  1 2</p><p class="P1"> </p><p class="P1">Now we can mount the new /var/lib/backuppc mount point</p><p class="P1"><span> </span></p><p class="P1"><span> </span><span> sudo mount /var/lib/backuppc</span></p><p class="P1"> </p><p class="P1">Voila! we now have the large storage volume mounted at /var/lib/backuppc</p><p class="P1"> </p><p class="P1">The var/lib/backuppc directory has root user root group set. We need to change</p><p class="P1">it.  We must adjust the new backuppc mountpoint to provide "backuppc" user and</p><p class="P1">group access.  This is why we could not mount directly earlier in the</p><p class="P1">process--there was no backuppc user and group at that point in time.</p><p class="P1"> </p><p class="P1"><span> sudo chown backuppc /var/lib/backuppc</span></p><p class="P1"><span> sudo chgrp backuppc /var/lib/backuppc</span></p><p class="P1"> </p><p class="P1">Umount and mount the fstab "backuppc" mount point. This is done to prove to</p><p class="P1">ourselves that we can maintain filesystem attributes through a mount.</p><p class="P1"> </p><p class="P1"><span> sudo umount /var/lib/backuppc</span></p><p class="P1"><span> sudo mount /var/lib/backuppc</span></p><p class="P1"> </p><p class="P1">Verify mounting, owner, group, and exec rights.</p><p class="P1"> </p><p class="P1"><span> ls -lah /var/lib/backuppc</span></p><p class="P1"> </p><p class="P1"><span> ....total 12K</span></p><p class="P1"><span> drwxr-xr-x  8 backuppc backuppc  118 Jan 20 11:25 .</span></p><p class="P1"><span> drwxr-xr-x 70 root     root     4.0K Jan 19 19:09 ..</span></p><p class="P1"><span> ....</span></p><p class="P1"><span> (note owner and group on the backuppc directory above in the "." entry</span></p><p class="P1"> </p><p class="P1">Start the backuppc daemon</p><p class="P1"> </p><p class="P1"><span> sudo /etc/init.d/backuppc start</span></p><p class="P1"> </p><p class="P1">We are in business! BackupPC should be up and running.</p><p class="P1"> </p><p class="P1">--------------------------------------------</p><p class="P1"> </p><p class="P1">--- Configuring the APC UPS daemon for automated power management</p><p class="P1"> </p><p class="P1">This section provide details on setting up an APC brand uninterupptable</p><p class="P1">power supply (UPS) for use with the BackupPC server. Most APC models</p><p class="P1">can simply be connected to the host using a USB cable--allowing the host</p><p class="P1">to monitor the UPS condition continuously and provide a soft system</p><p class="P1">shutdown when power goes down for an extended period. This control capability</p><p class="P1">required the apcupsd power supply daemon be installed and configured in</p><p class="P1">the host system.</p><p class="P1"> </p><p class="P1">Install the apcupsd power supply daemon.</p><p class="P1"> </p><p class="P1"><span> sudo apt-get install apcupsd</span></p><p class="P1"> </p><p class="P1">Follow instructions in the CentOS Hypervisor server hypervisor document for</p><p class="P1">configuration of apcupsd. These instructions cover both dedicated and shared</p><p class="P1">use of the UPS. Basically you need to edit the following:</p><p class="P1"> </p><p class="P1"><span> sudo gedit /etc/apcupsd/apcupsd.conf</span></p><p class="P1"> </p><p class="P1">Use the descriptions provided in the file to set the paramaters. For example</p><p class="P1">suppose you have a dedicated USB connected UPS:</p><p class="P1"> </p><p class="P1"><span> UPSNAME my-ups-name</span></p><p class="P1"><span> UPSCABLE usb</span></p><p class="P1"><span> etc...</span></p><p class="P1"> </p><p class="P1">Set the UPS as being configured in the apcupsd defaults</p><p class="P1"> </p><p class="P1"><span> vim /etc/default/apcupsd</span></p><p class="P1"> </p><p class="P1"><span> </span><span> ISCONFIGURED=yes</span></p><p class="P1"> </p><p class="P1">Afterwards, start the apdupsd daemon.</p><p class="P1"> </p><p class="P1"><span> sudo /etc/init.d/apcupsd start</span></p><p class="P1"> </p><p class="P1">Verify power supply connection and status using the apcaccess command.</p><p class="P1"> </p><p class="P1"><span> apcaccess</span></p><p class="P1"> </p><p class="P1">Output of apaccess will provide details about the connected power supply.</p><p class="P1">For example:</p><p class="P1"> </p><p class="P1"><span> APC      : 001,023,0596</span></p><p class="P1"><span> DATE     : 2015-02-09 15:38:52 -0500  </span></p><p class="P1"><span> HOSTNAME : m17p1-00</span></p><p class="P1"><span> VERSION  : 3.14.10 (13 September 2011) debian</span></p><p class="P1"><span> UPSNAME  : local-UPS-CS-350</span></p><p class="P1"><span> CABLE    : USB Cable</span></p><p class="P1"><span> DRIVER   : USB UPS Driver</span></p><p class="P1"><span> UPSMODE  : Stand Alone</span></p><p class="P1"><span> STARTTIME: 2015-02-09 15:04:33 -0500  </span></p><p class="P1"><span> MODEL    : Back-UPS 350 </span></p><p class="P1"><span> STATUS   : ONLINE </span></p><p class="P1"><span> BCHARGE  : 100.0 Percent</span></p><p class="P1"><span> TIMELEFT :  43.0 Minutes</span></p><p class="P1"><span> MBATTCHG : 10 Percent</span></p><p class="P1"><span> MINTIMEL : 5 Minutes</span></p><p class="P1"><span> MAXTIME  : 0 Seconds</span></p><p class="P1"><span> NUMXFERS : 0</span></p><p class="P1"><span> TONBATT  : 0 seconds</span></p><p class="P1"><span> CUMONBATT: 0 seconds</span></p><p class="P1"><span> XOFFBATT : N/A</span></p><p class="P1"><span> STATFLAG : 0x07000008 Status Flag</span></p><p class="P1"><span> SERIALNO : BB0323008746</span></p><p class="P1"><span> FIRMWARE : 5.1.D USB FW: c1 </span></p><p class="P1"><span> END APC  : 2015-02-09 15:38:59 -0500</span></p><p class="P1"> </p><p class="P1">--------------------------------------------</p><p class="P1"> </p><p class="P1">---notes on some alternative partitioning for the Linux Mint backuppc host:</p><p class="P1"> </p><p class="P1">An alternative and avoidance to using the "switcheroo" would be to mount the</p><p class="P1">large storage volume at "/var", as part of the original installation of Mint.</p><p class="P1">Unfortunately this would complicate the simple Mint installation procedure by</p><p class="P1">requiring a more complicated partitioning scheme using LVM with additional</p><p class="P1">volumes. The relative ease of using "system-config-lvm" was the predominant</p><p class="P1">draw and advantage for putting up with the tradeoff in this next section.</p><p class="P1"> </p><p class="P1">Yet another approach might suggest to mount the large LVM storage to /var</p><p class="P1">**after** the install of Mint. This method was not verified--**mileage may</p><p class="P1">vary**. We could run into other, more cumbersome, filesystem and ownership</p><p class="P1">issues with this approach--operating on a live system. It might possibly</p><p class="P1">require us to manage the filesystem using an alternative boot media. Do-able,</p><p class="P1">but we would rather avoid this sticky web.</p><p class="P1"> </p><p class="P1">Another approach would be obviously to just use one very large hard drive for</p><p class="P1">the Linux Mint backuppc system. This would be the most simple to implement for</p><p class="P1">sure. As long as LVM is used, adding another drive for expansion of storage</p><p class="P1">would be quite easy to accomplish. Again, we would use "system-config-lvm" as a</p><p class="P1">somewhat simple means to do this, but this would not have to be done until</p><p class="P1">backup storage space started to become an issue.</p><p class="P1"> </p><p class="P1">==========================================================================</p><p class="P1"> </p><p class="P1">--- Configuring backuppc host ssh access to client computers</p><p class="P1"> </p><p class="P1">To use host names for client hosts in backuppc, we need these client hostnames</p><p class="P1">defined to IP addresses.  If we have an available DNS server, we can add some</p><p class="P1">forward and reverse host records to get the job done.  Editing DNS records is</p><p class="P1">beyond the scope of this instruction set.</p><p class="P1"> </p><p class="P1">As an alternative to DNS, we can add the required definitions directly to</p><p class="P1">/etc/hosts at the backuppc host.  This will provide host name resolution within</p><p class="P1">the browser view of the backuppc website interface.  We would do this only if</p><p class="P1">we are not providing records for hostnames in a DNS server but want to use</p><p class="P1">named hosts in the backuppc website interface.</p><p class="P1"> </p><p class="P1"><span> $ sudo gedit /etc/hosts </span></p><p class="P1"> </p><p class="P1">Without hostnames defined we could forgo hostname usage and instead use direct</p><p class="P1">IP addresses to add backup clients to backuppc.</p><p class="P1"> </p><p class="P1">--- Create a public key pair for backuppc user</p><p class="P1"> </p><p class="P1">Access the backuppc host from remote client, or simple open a terminal on the</p><p class="P1">backup host Mint desktop.</p><p class="P1"> </p><p class="P1"><span> $ ssh user@ip_address</span></p><p class="P1"> </p><p class="P1">We will need a SSH key pair, public and private, for the backuppc user.  We</p><p class="P1">need to create this only **once** on the pc-backup host. The public key will be</p><p class="P1">sent to the hypervisor client hosts that we plan to provide backup services</p><p class="P1">for.</p><p class="P1"> </p><p class="P1">Change user, su to backuppc user...</p><p class="P1"> </p><p class="P1"><span> $ sudo su - backuppc</span></p><p class="P1"> </p><p class="P1">From backuppc user, not sudo, generate a key pair using the ssh-keygen</p><p class="P1">utility...</p><p class="P1"> </p><p class="P1"><span> $ ssh-keygen</span></p><p class="P1"> </p><p class="P1">Accept all defaults, we do not want a passphrase in this case.</p><p class="P1"> </p><p class="P1">From backuppc user, transfer the backuppc user's public SSH key to any client</p><p class="P1">host that will need to be accessed for backup purposes.  This will need to be</p><p class="P1">done **for each client system** that will need to be accessed for backup</p><p class="P1">purposes.</p><p class="P1"> </p><p class="P1"><span> $ ssh-copy-id root@client_name</span></p><p class="P1"> </p><p class="P1">You can use the fully qualified or short version of the name.  If you are not</p><p class="P1">using /etc/hosts names or DNS hostnames then, instead of the client_name, use</p><p class="P1">the direct IP address of the client.</p><p class="P1"> </p><p class="P1">The ssh-copy-id command must be done **for each** client system that require</p><p class="P1">backup services.</p><p class="P1"> </p><p class="P1">After sending the public key, check to be sure you can access the client, from</p><p class="P1">backuppc user, without a password.</p><p class="P1"> </p><p class="P1"><span> $ ssh root@client_name</span></p><p class="P1"> </p><p class="P1">You want to make sure to use the same form of the name you are planning to use</p><p class="P1">when you add this client host to BackupPC server. For example, if you are</p><p class="P1">planning to add the host as short name "my-hypervisor4", then use the same form</p><p class="P1">here.  Reason for this is the first time it is done it will add the server name</p><p class="P1">to the known hosts list for the backuppc user.  If you use a different form,</p><p class="P1">then you will get a preauth failure when the ssh job runs which looks like this</p><p class="P1">in CentOS /var/log/secure "Connection closed by 10.9.xxx.xxx [preauth]"</p><p class="P1"> </p><p class="P1">Exit from the backuppc user.</p><p class="P1"> </p><p class="P1"><span> $ exit</span></p><p class="P1"> </p><p class="P1">Exit from the remote host or terminal session.</p><p class="P1"> </p><p class="P1"><span> $ exit</span></p><p class="P1"> </p><p class="P1">==========================================================================</p><p class="P1"> </p><p class="P1">---Preparation of A CentOS 7 Hypervisor Client for backup via BackupPC</p><p class="P1"> </p><p class="P1">Here we will describe a typical backuppc setup for a CentOS 7 virtualization</p><p class="P1">server. This is a CentOS 7 hypervisor client host that will be backed up using</p><p class="P1">backuppc server.  Refer here, as an example, on how to set up CentOS 7 as a</p><p class="P1">hypervisor server. There are many other sources as well.</p><p class="P1"> </p><p class="P1">http://runlevel-6.github.io/blog/2014/12/11/centos-virtualization-server/</p><p class="P1"> </p><p class="P1">We have the vm-clone-dump.sh script copied into the directory below.  This</p><p class="P1">directory, and below, we will use for collection of data for backup purposes.</p><p class="P1">The virt-backup directory will be used for VM dump and VM dump cleanup of</p><p class="P1">before and after a backuppc job has completed.</p><p class="P1"> </p><p class="P1"><span> /var/lib/libvirt/images/virt-backup/virt-backup/.</span></p><p class="P1"><span> ...contains scripts, lockfile, log files</span></p><p class="P1"> </p><p class="P1"><span> /var/lib/libvirt/images/virt-backup/virt-backup/dump/.</span></p><p class="P1"><span> ...contains VM cloned images and VM XML definitions for both clone and</span></p><p class="P1"><span> original image.</span></p><p class="P1"><span> </span></p><p class="P1">--- dumping VMs to ready them for transfer to backuppc host.</p><p class="P1"> </p><p class="P1">The "vm-clone-dump.sh" script is called as a *pre-command* by the backuppc</p><p class="P1">process.</p><p class="P1"> </p><p class="P1"><span> vm-clone-dump.sh --dump</span></p><p class="P1"> </p><p class="P1">This creates a the dump directory and then creates temporary clones of running</p><p class="P1">and inactive VMs which are are placed in the dump directory.  It also copies</p><p class="P1">the cloned VM XML and original VM XML definition files into the dump directory</p><p class="P1">before exitting. Each VM is cloned into a folder named the same as the VM name.</p><p class="P1">The clone image can be used with the original XML to restore and/or move a VM.</p><p class="P1"> </p><p class="P1">--- cleaning up the VM dump after files have been transferred to backuppc</p><p class="P1"> </p><p class="P1">The "vm-clone-dump.sh" script is called as a *post-command* by the backuppc</p><p class="P1">process.</p><p class="P1"> </p><p class="P1"><span> vm-clone-dump.sh --clean</span></p><p class="P1"> </p><p class="P1">This cleans up the dump directory area, deletes the dump directory, and readies</p><p class="P1">the host for another dump operation.  These files can be used to recreate the</p><p class="P1">original VM with the same data at the time it was dumped.</p><p class="P1"> </p><p class="P1">We created, on the hypervisor client, the planned VM backup prep area here.</p><p class="P1"> </p><p class="P1"><span> /var/lib/libvirt/imgages/virt-backup</span></p><p class="P1"> </p><p class="P1">This location was chosen because on our hypervisor clients we have a large LVM</p><p class="P1">partition mounted at /var/lib/libvirt/images.  We want to use part of the same</p><p class="P1">LVM partion to store local VM dumps in preparation for transfer to backup</p><p class="P1">storage.  We are logged into the client system as root.</p><p class="P1"> </p><p class="P1">Future considerations: The choice of this directory was due to the large LVM</p><p class="P1">storage volume on the hypervisor being mounted at /var/lib/libvirt/images/.  An</p><p class="P1">improvement, at this point, might to be instead mount the large LVM storage</p><p class="P1">volume in the hypervisor instead to the /var directory This would permit a more</p><p class="P1">attractive and independent path for VM backups, such as /var/backuppc/.  This</p><p class="P1">would allow the backuppc scripts and dump area to be established in a separate</p><p class="P1">directory area independent of the libvirt specific sub-path.  Future iterations</p><p class="P1">of the hypervisor installation could be implemented in this way allowing for</p><p class="P1">the following:</p><p class="P1"> </p><p class="P1"><span> /var (large LVM storage volume)</span></p><p class="P1"><span> /var/lib/libvirt/images (default libvirt related VM images)</span></p><p class="P1"><span> /var/lib/libvirt/iso-images (libvirt related iso-images)</span></p><p class="P1"><span> /var/backuppc (backuppc related scripting)</span></p><p class="P1"><span> /var/backuppc/dump (backuppc VM dump data)</span></p><p class="P1"> </p><p class="P1">==========================================================================</p><p class="P1"> </p><p class="P1">---Tuning the solution to fit the hypervisor capabilities and loading</p><p class="P1"> </p><p class="P1">Through testing on several server systems I have found that running the VM dump</p><p class="P1">operation sequentially on each running VM, while other VMs are running, can</p><p class="P1">cause tremendous strain on a hypervisor resulting in VMs operating **very</p><p class="P1">slowly** during the dump operation. On older server systems this could look</p><p class="P1">like a broken server.  This issue depends on the loading and capabilities of</p><p class="P1">the hypervisor.  You get very different capabilities between an older desktop</p><p class="P1">system or older Dell 2950 and a brand shiny new Cisco blade with 32 Xeon cores,</p><p class="P1">1TB RAM, and full solid-state RAID storage system.  The 2 options below can be</p><p class="P1">used to provide regular backup data with the tradeoff of halting VM usability</p><p class="P1">at the hypervisor for the time it takes to dump the VM data.</p><p class="P1"> </p><p class="P1">---Option: Disable all automated backups.</p><p class="P1"> </p><p class="P1">Disable all host_name automated backups and require the use of admin "backuppc"</p><p class="P1">user manual backup request only.  This essentially means that when a client</p><p class="P1">host backup is needed, the backuppc user must go into the website and request a</p><p class="P1">host backup.</p><p class="P1"> </p><p class="P1">host_name - Edit Config - Schedule</p><p class="P1"> </p><p class="P1"><span> BackupsDisable: 1</span></p><p class="P1"> </p><p class="P1">---Option: Throughput Increase for Clone Dump Processing</p><p class="P1"> </p><p class="P1">1. The vm-clone-dump.sh will, by default, pause **all** running VMs in the</p><p class="P1">hypervisor before creating dumped clones.  Preventing concurrent use of VMs</p><p class="P1">while dumping may shorten the dump processing time in the hypervisor.  This</p><p class="P1">however will prevent use of VMs while the hypervisor dump is being prepared and</p><p class="P1">backup files transferred--a reasonable tradoff.</p><p class="P1"> </p><p class="P1">2. Change vm-clone-dump.sh processing to be --concurrent, such that each VMs is</p><p class="P1">individually paused, and then resumed after it is cloned and dumped.  This will</p><p class="P1">generally improve the uptime of VMs during a hypervisor clone dump</p><p class="P1">operation--assuming the server can handle the load.  The tradoff may be a</p><p class="P1">degradation of performance on VMs during the clone creation--VM users may</p><p class="P1">become frustrated. This approach is not recommended on older servers with</p><p class="P1">relatively slower disk IO. Newer solid state storage may be required here to</p><p class="P1">obtain the desired concurrent effect while keeping the other VMs humming along.</p><p class="P1"> </p><p class="P1">==========================================================================</p><p class="P1"> </p><p class="P1">---BackupPC Administrative website interface</p><p class="P1"> </p><p class="P1">Browser access to backuppc administration site to add and configure clients</p><p class="P1"> </p><p class="P1">Access the backup host using HTTP from a remote client with a browser.  You</p><p class="P1">could also access backuppc administration directly from the Mint desktop using</p><p class="P1">Firefox.</p><p class="P1"> </p><p class="P1"><span> http://host_name_or_ip_address/backuppc</span></p><p class="P1"> </p><p class="P1"><span> </span><span> provide credentials:</span></p><p class="P1"><span> </span><span> </span><span> user: backuppc</span></p><p class="P1"><span> </span><span> </span><span> pass: the_password</span></p><p class="P1"> </p><p class="P1">Edit Hosts - Delete "localhost"</p><p class="P1"> </p><p class="P1">Remove the localhost from the backup configuration.</p><p class="P1"> </p><p class="P1">Once you have deleted the localhost from backuppc administration you can then</p><p class="P1">delete the localhost backup files from the backuppc host.</p><p class="P1"> </p><p class="P1">From the backuppc host we can remove the localhost backup files.</p><p class="P1"> </p><p class="P1"><span> $ sudo rm -rf /var/lib/backuppc/pc/localhost</span></p><p class="P1"> </p><p class="P1">Before we can add any hypervisor host clients we need to prepare them using the</p><p class="P1">CentOS client instructions above as a working example.</p><p class="P1"> </p><p class="P1">In the backuppc administrative interface we will want to add the hypervisor</p><p class="P1">host clients we want to have backed up.</p><p class="P1"> </p><p class="P1"><span> Edit Hosts - Add button</span></p><p class="P1"> </p><p class="P1">Provide the name of the hypervisor client host and then "Save". Keep in mind,</p><p class="P1">if you are using names, then you need to have either a DNS server entry or a</p><p class="P1">hosts file entry as detailed above in BackupPC setup instructions.</p><p class="P1"> </p><p class="P1"><span> Choose the dropdown "Select a host..."</span></p><p class="P1"> </p><p class="P1">Now you can select the host from the pull down in the upper left of the</p><p class="P1">administration website.  This will bring the new host into context.  Now you</p><p class="P1">can edit the client host specific config by selecting the "Edit Config" link</p><p class="P1">directly below the host in context.</p><p class="P1"> </p><p class="P1">All of the configuration for the host is available globally in the general host</p><p class="P1">settings. Editing a specific host configuration can be done when you need to</p><p class="P1">configure specific parameters for a given client host. By creating a</p><p class="P1">configuration for the system, each host can then use the same configuration.</p><p class="P1"> </p><p class="P1">Host - Edit Config</p><p class="P1"> </p><p class="P1">Host config "Xfer" tab settings are as follows:</p><p class="P1"> </p><p class="P1"><span> XferMethod: rsync</span></p><p class="P1"><span> RSyncShareName: /var/lib/libvirt/images/virt-backup</span></p><p class="P1"> </p><p class="P1">Host config "Backup Settings" tab settings</p><p class="P1"> </p><p class="P1"><span> DumpPreUserCmd:</span></p><p class="P1"> </p><p class="P1"> $sshPath -q -x -l root $host /var/lib/libvirt/images/virt-backup/vm-clone-dump.sh --dump</p><p class="P1"> </p><p class="P1"><span> DumpPostUserCmd:</span></p><p class="P1"> </p><p class="P1"> $sshPath -q -x -l root $host /var/lib/libvirt/images/virt-backup/vm-clone-dump.sh --clean</p><p class="P1"> </p><p class="P1">Most of the other settings can remain as default. Schedule can be reviewed and</p><p class="P1">adjusted as necessary.  We are now ready to start a full backup.</p><p class="P1"> </p><p class="P1">Select the "hostname Home" then select "Start Full Backup".  Check the host and</p><p class="P1">server logfiles for errors and/or progress.  The backup should complete.  It</p><p class="P1">may take an hour or more, depending on the number of VMs, throughput of the</p><p class="P1">hypervisor client, etc.</p><p class="P1"> </p><p class="P1">---------------------------------------------------------------------</p><p class="P1">- end of document</p></body></html>